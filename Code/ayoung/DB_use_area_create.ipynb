{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "assigned-science",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "documented-proxy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/09 10:12:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/09/09 10:12:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "#connector 설정\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local') \\\n",
    "    .setAppName('sql') \\\n",
    "    .set(\"spark.driver.extraClassPath\",\"jdbc/mysql-connector-java-5.1.44.jar\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "laden-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#db접속정보 설정\n",
    "                        #[ip주소:port]   #[db명]                 #[유저명]          #[비밀번호]\n",
    "DB_URL = 'jdbc:mysql://                /       ?useSSL=false&user=        &password=           '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sweet-conditions",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+----+--------+--------+--------+--------+----------+\n",
      "|소재지(시군구)별(1)|소재지(시군구)별(2)|시점|주거지역|상업지역|공업지역|녹지지역|미지정지역|\n",
      "+-------------------+-------------------+----+--------+--------+--------+--------+----------+\n",
      "|소재지(시군구)별(1)|소재지(시군구)별(2)|시점|비율 (%)|비율 (%)|비율 (%)|비율 (%)|  비율 (%)|\n",
      "|               전국|               소계|2017|   15.14|    1.88|    6.70|   71.54|      4.75|\n",
      "|               전국|               소계|2018|   15.09|    1.88|    6.74|   70.99|      5.31|\n",
      "|               전국|               소계|2019|   15.20|    1.90|    6.86|   71.11|      4.92|\n",
      "|               전국|               소계|2020|   15.27|    1.90|    6.88|   71.04|      4.91|\n",
      "|         서울특별시|               소계|2017|   53.82|    4.18|    3.30|   38.71|      0.00|\n",
      "|         서울특별시|               소계|2018|   53.69|    4.23|    3.30|   38.78|      0.00|\n",
      "|         서울특별시|               소계|2019|   53.68|    4.24|    3.30|   38.78|      0.00|\n",
      "|         서울특별시|               소계|2020|   53.78|    4.24|    3.28|   38.69|      0.00|\n",
      "|         서울특별시|             종로구|2017|   41.01|   12.25|    0.00|   46.74|      0.00|\n",
      "+-------------------+-------------------+----+--------+--------+--------+--------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#데이터파일(csv)을 DataFrame 으로 읽어들인다.\n",
    "# inferSchema : 데이터를 기반으로 타입을 유추(기본값 : False)\n",
    "# header : 첫번째 행을 컬럼 값으로 사용\n",
    "\n",
    "df = spark.read.csv(\"data/2017_2020_녹지율.csv\", encoding = 'cp949',inferSchema = True, header = True)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bigger-restoration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[소재지(시군구)별(1): string, 소재지(시군구)별(2): string, 시점: string, 주거지역: string, 상업지역: string, 공업지역: string, 녹지지역: string, 미지정지역: string]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#컬럼정보 확인\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bottom-width",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# DataFrame 타입인 것을 확인\n",
    "print(type(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "challenging-token",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#DB에 저장\n",
    "df \\\n",
    "    .write \\\n",
    "    .format('jdbc') \\\n",
    "    .mode('append') \\\n",
    "    .option(\"url\", DB_URL) \\\n",
    "    .option(\"dbtable\", \"use_area2017_2020\") \\\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lab04]",
   "language": "python",
   "name": "conda-env-lab04-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
