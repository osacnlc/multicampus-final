{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "light-stability",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "assigned-science",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import *\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "documented-proxy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/08 17:13:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#connector 설정\n",
    "\n",
    "conf = SparkConf() \\\n",
    "    .setMaster('local') \\\n",
    "    .setAppName('sql') \\\n",
    "    .set(\"spark.driver.extraClassPath\",\"jdbc/mysql-connector-java-5.1.44.jar\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "laden-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#db접속정보 설정\n",
    "                        #[ip주소:port]   #[db명]                 #[유저명]          #[비밀번호]\n",
    "DB_URL = 'jdbc:mysql://                /       ?useSSL=false&user=        &password=           '\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-sierra",
   "metadata": {},
   "source": [
    "1.가구유형데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sweet-conditions",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-------------+---------------+----+--------+------------+------------+------------+--------------+-------+\n",
      "|      시도|행정구역별(시군구)|가구주의 성별|가구주의 연령별|시점|일반가구|1세대가구-계|2세대가구-계|3세대가구-계|4세대이상 가구|1인가구|\n",
      "+----------+------------------+-------------+---------------+----+--------+------------+------------+------------+--------------+-------+\n",
      "|서울특별시|        서울특별시|           계|           합계|2017| 3813260|      614037|     1775431|      173908|          1297|1180540|\n",
      "|서울특별시|        서울특별시|           계|           합계|2018| 3839766|      626988|     1740777|      164734|          1121|1229421|\n",
      "|서울특별시|        서울특별시|           계|           합계|2019| 3896389|      645266|     1710502|      155420|          1020|1299787|\n",
      "|서울특별시|        서울특별시|           계|           합계|2020| 3982290|      665193|     1691450|      142292|           767|1390701|\n",
      "|서울특별시|            종로구|           계|           합계|2017|   62372|        9986|       24272|        2839|            23|  23638|\n",
      "+----------+------------------+-------------+---------------+----+--------+------------+------------+------------+--------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#데이터파일(csv)을 DataFrame 으로 읽어들인다.\n",
    "# inferSchema : 데이터를 기반으로 타입을 유추(기본값 : False)\n",
    "# header : 첫번째 행을 컬럼 값으로 사용\n",
    "\n",
    "df1 = spark.read.csv(\"data/2017_2020_가구유형.csv\",encoding='cp949', inferSchema = True, header = True)\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bigger-restoration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[시도: string, 행정구역별(시군구): string, 가구주의 성별: string, 가구주의 연령별: string, 시점: int, 일반가구: int, 1세대가구-계: int, 2세대가구-계: int, 3세대가구-계: int, 4세대이상 가구: string, 1인가구: int]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#컬럼정보 확인\n",
    "df1.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bottom-width",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# DataFrame 타입인 것을 확인\n",
    "print(type(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "challenging-token",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#전체데이터 DB에 저장\n",
    "\n",
    "df1 \\\n",
    "    .write \\\n",
    "    .format('jdbc') \\\n",
    "    .mode('append') \\\n",
    "    .option(\"url\", DB_URL) \\\n",
    "    .option(\"dbtable\", \"family2017_2020\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-armenia",
   "metadata": {},
   "source": [
    "2.건축연도데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "revolutionary-fellowship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+----+-------------+--------+------+------+------+------+------+------+------+------+------+------+------+-----------+-----------+-----------+-----------+-----------+\n",
      "|      시도|행정구역별(시군구)|시점|주택의 종류별|연면적별|2020년|2019년|2018년|2017년|2016년|2015년|2014년|2013년|2012년|2011년|2010년|2005~2009년|2000~2004년|1990~1999년|1980~1989년|1979년 이전|\n",
      "+----------+------------------+----+-------------+--------+------+------+------+------+------+------+------+------+------+------+------+-----------+-----------+-----------+-----------+-----------+\n",
      "|서울특별시|        서울특별시|2017|           계|    합계|     0|     0|     0| 52138| 85696| 71833| 81708| 63132| 67466| 68643| 50016|     300432|     596244|     818777|     432556|     178204|\n",
      "|서울특별시|        서울특별시|2018|           계|    합계|     0|     0| 44662| 61493| 86546| 72230| 81740| 62582| 67502| 68084| 50036|     299832|     594473|     812594|     421439|     170865|\n",
      "|서울특별시|            종로구|2017|           계|    합계|     0|     0|     0|   484|   302|   469|   532|   902|   404|   258|   208|       4755|       9558|      14335|       4683|       8897|\n",
      "|서울특별시|            종로구|2018|           계|    합계|     0|     0|   206|   530|   302|   474|   549|   904|   404|   258|   209|       4752|       9540|      14302|       4634|       8665|\n",
      "|서울특별시|              중구|2017|           계|    합계|     0|     0|     0|   232|  1080|   427|   905|   470|   890|  2608|  1043|       4065|      10844|       9400|       2065|       5483|\n",
      "+----------+------------------+----+-------------+--------+------+------+------+------+------+------+------+------+------+------+------+-----------+-----------+-----------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2017_2020_시군구_건축연도 : 17,18(시점)\n",
    "df2_1 = spark.read.csv(\"data/2017_2020_시군구_건축연도_1718.csv\",encoding='cp949', inferSchema = True, header = True)\n",
    "df2_1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "automatic-durham",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.printSchema of DataFrame[시도: string, 행정구역별(시군구): string, 시점: int, 주택의 종류별: string, 연면적별: string, 2020년: int, 2019년: int, 2018년: int, 2017년: int, 2016년: int, 2015년: int, 2014년: int, 2013년: int, 2012년: int, 2011년: int, 2010년: int, 2005~2009년: int, 2000~2004년: int, 1990~1999년: int, 1980~1989년: int, 1979년 이전: int]> \n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#컬럼정보 확인\n",
    "print(df2_1.printSchema,'\\n')\n",
    "# DataFrame 타입인 것을 확인\n",
    "print(type(df2_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecological-shore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+----+-------------+--------+------+------+------+------+------+------+------+------+------+------+------+-----------+-----------+-----------+-----------+-----------+\n",
      "|      시도|행정구역별(시군구)|시점|주택의 종류별|연면적별|2020년|2019년|2018년|2017년|2016년|2015년|2014년|2013년|2012년|2011년|2010년|2005~2009년|2000~2004년|1990~1999년|1980~1989년|1979년 이전|\n",
      "+----------+------------------+----+-------------+--------+------+------+------+------+------+------+------+------+------+------+------+-----------+-----------+-----------+-----------+-----------+\n",
      "|서울특별시|        서울특별시|2019|           계|    합계|     0| 65363| 73312| 68328| 86910| 71212| 81695| 62927| 67665| 68145| 50248|     299124|     592177|     806133|     400831|     159894|\n",
      "|서울특별시|            종로구|2019|           계|    합계|     0|   625|   240|  2892|   303|   474|   548|   792|   353|   257|   222|       4503|       9081|      13446|       4515|       8189|\n",
      "|서울특별시|              중구|2019|           계|    합계|     0|  1261|   719|   818|  1082|   434|   904|   470|   884|  2605|  1041|       4060|      10826|       9381|       2033|       5236|\n",
      "|서울특별시|            용산구|2019|           계|    합계|     0|  1103|   816|  1391|   671|  1197|  1053|  1041|   779|  2777|   923|       7999|      13995|      17037|       7673|      14008|\n",
      "|서울특별시|            성동구|2019|           계|    합계|     0|  1677|  2258|  1964|  6529|  1998|  2073|   882|  4705|  1333|   168|       6367|      24667|      22841|       9004|       4571|\n",
      "+----------+------------------+----+-------------+--------+------+------+------+------+------+------+------+------+------+------+------+-----------+-----------+-----------+-----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#2017_2020_시군구_건축연도 : 시점 19,20\n",
    "df2_2 = spark.read.csv(\"data/2017_2020_시군구_건축연도_1920.csv\",encoding='cp949', inferSchema = True, header = True)\n",
    "df2_2.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "accessible-plymouth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.printSchema of DataFrame[시도: string, 행정구역별(시군구): string, 시점: int, 주택의 종류별: string, 연면적별: string, 2020년: int, 2019년: int, 2018년: int, 2017년: int, 2016년: int, 2015년: int, 2014년: int, 2013년: int, 2012년: int, 2011년: int, 2010년: int, 2005~2009년: int, 2000~2004년: int, 1990~1999년: int, 1980~1989년: int, 1979년 이전: int]> \n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#컬럼정보 확인\n",
    "print(df2_2.printSchema,'\\n')\n",
    "# DataFrame 타입인 것을 확인\n",
    "print(type(df2_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "derived-watch",
   "metadata": {},
   "outputs": [],
   "source": [
    "#시점 17/18 데이터 DB저장\n",
    "df2_1 \\\n",
    "    .write \\\n",
    "    .format('jdbc') \\\n",
    "    .mode('append') \\\n",
    "    .option(\"url\", DB_URL) \\\n",
    "    .option(\"dbtable\", \"building2017_2020_1\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "invisible-enforcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "#시점 19/20 데이터 DB저장\n",
    "df2_2 \\\n",
    "    .write \\\n",
    "    .format('jdbc') \\\n",
    "    .mode('append') \\\n",
    "    .option(\"url\", DB_URL) \\\n",
    "    .option(\"dbtable\", \"building2017_2020_2\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "champion-ordinary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+----+-------------+--------+------+------+------+------+------+------+------+------+------+------+------+-----------+-----------+-----------+-----------+-----------+\n",
      "|      시도|행정구역별(시군구)|시점|주택의 종류별|연면적별|2020년|2019년|2018년|2017년|2016년|2015년|2014년|2013년|2012년|2011년|2010년|2005~2009년|2000~2004년|1990~1999년|1980~1989년|1979년 이전|\n",
      "+----------+------------------+----+-------------+--------+------+------+------+------+------+------+------+------+------+------+------+-----------+-----------+-----------+-----------+-----------+\n",
      "|서울특별시|        서울특별시|2017|           계|    합계|     0|     0|     0| 52138| 85696| 71833| 81708| 63132| 67466| 68643| 50016|     300432|     596244|     818777|     432556|     178204|\n",
      "|서울특별시|        서울특별시|2018|           계|    합계|     0|     0| 44662| 61493| 86546| 72230| 81740| 62582| 67502| 68084| 50036|     299832|     594473|     812594|     421439|     170865|\n",
      "|서울특별시|            종로구|2017|           계|    합계|     0|     0|     0|   484|   302|   469|   532|   902|   404|   258|   208|       4755|       9558|      14335|       4683|       8897|\n",
      "+----------+------------------+----+-------------+--------+------+------+------+------+------+------+------+------+------+------+------+-----------+-----------+-----------+-----------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#시점 17-20 하나로 합친 데이터 만들기\n",
    "def unionAll(*dfs):\n",
    "    return reduce(DataFrame.union, dfs)\n",
    "\n",
    "df2_3 = unionAll(df2_1, df2_2)  \n",
    "df2_3.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "retained-pierce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.printSchema of DataFrame[시도: string, 행정구역별(시군구): string, 시점: int, 주택의 종류별: string, 연면적별: string, 2020년: int, 2019년: int, 2018년: int, 2017년: int, 2016년: int, 2015년: int, 2014년: int, 2013년: int, 2012년: int, 2011년: int, 2010년: int, 2005~2009년: int, 2000~2004년: int, 1990~1999년: int, 1980~1989년: int, 1979년 이전: int]> \n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#컬럼정보 확인\n",
    "print(df2_3.printSchema,'\\n')\n",
    "# DataFrame 타입인 것을 확인\n",
    "print(type(df2_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adjustable-charles",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 13:=============================>                            (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#시점 17-20 합친 데이터 DB저장\n",
    "df2_3 \\\n",
    "    .write \\\n",
    "    .format('jdbc') \\\n",
    "    .mode('append') \\\n",
    "    .option(\"url\", DB_URL) \\\n",
    "    .option(\"dbtable\", \"building2017_2020_3\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-bonus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-madonna",
   "metadata": {},
   "source": [
    "3.성/연령데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "wanted-keeping",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/08 17:13:30 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+-------+---------+---------+------+---------+---------+-------+---------+---------+-------+---------+---------+---------+---------+-------+---------+---------+---------+---------+---------+---------+-----------+---------+---------+---------+---------+----------+-----------+--------+----+----+----+----+----+----+----+----+----+----+\n",
      "|연도|  시도|시군구|0 - 4세|5 - 9세|10 - 14세|15 - 19세|0-19세|20 - 24세|25 - 29세|20-29세|30 - 34세|35 - 39세|30-39세|40 - 44세|45 - 49세|50 - 54세|55 - 59세|40-59세|60 - 64세|65 - 69세|70 - 74세|75 - 79세|60세-79세|80 - 84세|80세 이상25|85 - 89세|85세 이상|90 - 94세|95 - 99세|100세 이상|80세 이상31|  총합계|_c33|_c34|_c35|_c36|_c37|_c38|_c39|_c40|_c41|_c42|\n",
      "+----+------+------+-------+-------+---------+---------+------+---------+---------+-------+---------+---------+-------+---------+---------+---------+---------+-------+---------+---------+---------+---------+---------+---------+-----------+---------+---------+---------+---------+----------+-----------+--------+----+----+----+----+----+----+----+----+----+----+\n",
      "|2017|강원도|강릉시|   6810|   8495|     9526|    13060| 37890|    14439|    10429|  24868|    10350|    13630|  23980|    16255|    18199|    17046|    19682|  71181|    15874|    10984|     9950|     8536|    45343|     5130|       8868|     2507|     3738|      973|      223|        36|      21473|224734.0|null|null|null|null|null|null|null|null|null|null|\n",
      "|2017|강원도|고성군|    729|    784|      939|     1448|  3899|     2526|     1880|   4406|     1216|     1287|   2502|     1613|     2240|     2381|     3054|   9287|     2574|     1772|     1908|     1752|     8005|      962|       1757|      509|      795|      227|       53|         7|       4308| 32406.0|null|null|null|null|null|null|null|null|null|null|\n",
      "|2017|강원도|동해시|   3584|   4233|     4469|     5708| 17992|     5743|     4056|   9799|     4719|     6448|  11166|     7489|     8232|     7362|     8273|  31355|     6723|     4573|     4039|     3416|    18750|     1982|       3265|      919|     1284|      297|       59|         9|       7814| 96875.5|null|null|null|null|null|null|null|null|null|null|\n",
      "+----+------+------+-------+-------+---------+---------+------+---------+---------+-------+---------+---------+-------+---------+---------+---------+---------+-------+---------+---------+---------+---------+---------+---------+-----------+---------+---------+---------+---------+----------+-----------+--------+----+----+----+----+----+----+----+----+----+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/08 17:13:30 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 연도, 시도, 시군구, 0 - 4세, 5 - 9세, 10 - 14세, 15 - 19세, 0-19세, 20 - 24세, 25 - 29세, 20-29세, 30 - 34세, 35 - 39세, 30-39세, 40 - 44세, 45 - 49세, 50 - 54세, 55 - 59세, 40-59세, 60 - 64세, 65 - 69세, 70 - 74세, 75 - 79세, 60세-79세, 80 - 84세, 80세 이상, 85 - 89세, 85세 이상, 90 - 94세, 95 - 99세, 100세 이상, 80세 이상, 총합계, , , , , , , , , , \n",
      " Schema: 연도, 시도, 시군구, 0 - 4세, 5 - 9세, 10 - 14세, 15 - 19세, 0-19세, 20 - 24세, 25 - 29세, 20-29세, 30 - 34세, 35 - 39세, 30-39세, 40 - 44세, 45 - 49세, 50 - 54세, 55 - 59세, 40-59세, 60 - 64세, 65 - 69세, 70 - 74세, 75 - 79세, 60세-79세, 80 - 84세, 80세 이상25, 85 - 89세, 85세 이상, 90 - 94세, 95 - 99세, 100세 이상, 80세 이상31, 총합계, _c33, _c34, _c35, _c36, _c37, _c38, _c39, _c40, _c41, _c42\n",
      "Expected: 80세 이상25 but found: 80세 이상\n",
      "CSV file: file:///home/lab04/data/2017_2020_연령22.csv\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.read.csv(\"data/2017_2020_연령22.csv\",encoding='cp949', inferSchema = True, header = True)\n",
    "df3.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "thousand-briefs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.printSchema of DataFrame[연도: int, 시도: string, 시군구: string, 0 - 4세: int, 5 - 9세: int, 10 - 14세: int, 15 - 19세: int, 0-19세: int, 20 - 24세: int, 25 - 29세: int, 20-29세: int, 30 - 34세: int, 35 - 39세: int, 30-39세: int, 40 - 44세: int, 45 - 49세: int, 50 - 54세: int, 55 - 59세: int, 40-59세: int, 60 - 64세: int, 65 - 69세: int, 70 - 74세: int, 75 - 79세: int, 60세-79세: int, 80 - 84세: int, 80세 이상25: int, 85 - 89세: int, 85세 이상: int, 90 - 94세: int, 95 - 99세: int, 100세 이상: int, 80세 이상31: int, 총합계: double, _c33: string, _c34: string, _c35: string, _c36: string, _c37: string, _c38: string, _c39: string, _c40: string, _c41: string, _c42: string]> \n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#컬럼정보 확인\n",
    "print(df3.printSchema,'\\n')\n",
    "# DataFrame 타입인 것을 확인\n",
    "print(type(df3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fixed-brain",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/08 17:13:30 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 연도, 시도, 시군구, 0 - 4세, 5 - 9세, 10 - 14세, 15 - 19세, 0-19세, 20 - 24세, 25 - 29세, 20-29세, 30 - 34세, 35 - 39세, 30-39세, 40 - 44세, 45 - 49세, 50 - 54세, 55 - 59세, 40-59세, 60 - 64세, 65 - 69세, 70 - 74세, 75 - 79세, 60세-79세, 80 - 84세, 80세 이상, 85 - 89세, 85세 이상, 90 - 94세, 95 - 99세, 100세 이상, 80세 이상, 총합계, , , , , , , , , , \n",
      " Schema: 연도, 시도, 시군구, 0 - 4세, 5 - 9세, 10 - 14세, 15 - 19세, 0-19세, 20 - 24세, 25 - 29세, 20-29세, 30 - 34세, 35 - 39세, 30-39세, 40 - 44세, 45 - 49세, 50 - 54세, 55 - 59세, 40-59세, 60 - 64세, 65 - 69세, 70 - 74세, 75 - 79세, 60세-79세, 80 - 84세, 80세 이상25, 85 - 89세, 85세 이상, 90 - 94세, 95 - 99세, 100세 이상, 80세 이상31, 총합계, _c33, _c34, _c35, _c36, _c37, _c38, _c39, _c40, _c41, _c42\n",
      "Expected: 80세 이상25 but found: 80세 이상\n",
      "CSV file: file:///home/lab04/data/2017_2020_연령22.csv\n"
     ]
    }
   ],
   "source": [
    "#DB저장\n",
    "df3 \\\n",
    "    .write \\\n",
    "    .format('jdbc') \\\n",
    "    .mode('append') \\\n",
    "    .option(\"url\", DB_URL) \\\n",
    "    .option(\"dbtable\", \"age2017_2020\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-paint",
   "metadata": {},
   "source": [
    "4.흡연율데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "special-disclaimer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+------+\n",
      "|시점|       _c1|행정구역별|데이터|\n",
      "+----+----------+----------+------+\n",
      "|2017|서울특별시|서울특별시|  20.0|\n",
      "|2017|서울특별시|    종로구|  16.5|\n",
      "|2017|서울특별시|      중구|  20.8|\n",
      "+----+----------+----------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/08 17:13:31 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 시점, , 행정구역별, 데이터\n",
      " Schema: 시점, _c1, 행정구역별, 데이터\n",
      "Expected: _c1 but found: \n",
      "CSV file: file:///home/lab04/data/2017_2020_흡연율.csv\n"
     ]
    }
   ],
   "source": [
    "df4 = spark.read.csv(\"data/2017_2020_흡연율.csv\",encoding='cp949', inferSchema = True, header = True)\n",
    "df4.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "twenty-prince",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.printSchema of DataFrame[시점: int, _c1: string, 행정구역별: string, 데이터: double]> \n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#컬럼정보 확인\n",
    "print(df4.printSchema,'\\n')\n",
    "# DataFrame 타입인 것을 확인\n",
    "print(type(df4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "educated-recall",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/08 17:13:31 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: 시점, , 행정구역별, 데이터\n",
      " Schema: 시점, _c1, 행정구역별, 데이터\n",
      "Expected: _c1 but found: \n",
      "CSV file: file:///home/lab04/data/2017_2020_흡연율.csv\n"
     ]
    }
   ],
   "source": [
    "#DB저장\n",
    "df4 \\\n",
    "    .write \\\n",
    "    .format('jdbc') \\\n",
    "    .mode('append') \\\n",
    "    .option(\"url\", DB_URL) \\\n",
    "    .option(\"dbtable\", \"smoking2017_2020\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-conviction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-curve",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lab04]",
   "language": "python",
   "name": "conda-env-lab04-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
